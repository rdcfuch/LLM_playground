{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNtCDuxs1fgT",
        "outputId": "da433728-460a-4440-aa1c-1cad7426094a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n"
          ]
        }
      ],
      "source": [
        "#请 修改->笔记本设置->T4 GPU,然后确认下方输出Tesla T4来确认有显卡\n",
        "#然后点击 代码执行工具->全部运行 等待约五到十分钟\n",
        "#最后点击 最后下方显示的的链接\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4XrTo4N1g7N",
        "outputId": "ca43ccce-de9f-4f02-c7e0-dc4805ce54eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.12.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (4.7.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Using cached fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.4 (from gradio)\n",
            "  Downloading gradio_client-1.5.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (2.5.2)\n",
            "Requirement already satisfied: pydub in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (0.0.18)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.9.2-py3-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio) (0.30.6)\n",
            "Requirement already satisfied: fsspec in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio-client==1.5.4->gradio) (2024.9.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from gradio-client==1.5.4->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Using cached starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.25.1->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.25.1->gradio) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from pydantic>=2.0->gradio) (2.14.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (1.26.16)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.12.0-py3-none-any.whl (57.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.4-py3-none-any.whl (321 kB)\n",
            "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Using cached fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl (18 kB)\n",
            "Downloading ruff-0.9.2-py3-none-macosx_11_0_arm64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Using cached starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Installing collected packages: tomlkit, semantic-version, ruff, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 24.1.0\n",
            "    Uninstalling aiofiles-24.1.0:\n",
            "      Successfully uninstalled aiofiles-24.1.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.37.2\n",
            "    Uninstalling starlette-0.37.2:\n",
            "      Successfully uninstalled starlette-0.37.2\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.111.0\n",
            "    Uninstalling fastapi-0.111.0:\n",
            "      Successfully uninstalled fastapi-0.111.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "open-webui 0.5.2 requires chromadb==0.5.15, but you have chromadb 0.4.22 which is incompatible.\n",
            "open-webui 0.5.2 requires fastapi==0.111.0, but you have fastapi 0.115.6 which is incompatible.\n",
            "open-webui 0.5.2 requires langchain==0.3.7, but you have langchain 0.2.17 which is incompatible.\n",
            "open-webui 0.5.2 requires langchain-community==0.3.7, but you have langchain-community 0.2.19 which is incompatible.\n",
            "open-webui 0.5.2 requires nltk==3.9.1, but you have nltk 3.8.1 which is incompatible.\n",
            "open-webui 0.5.2 requires pydantic==2.9.2, but you have pydantic 2.5.2 which is incompatible.\n",
            "open-webui 0.5.2 requires pymilvus==2.5.0, but you have pymilvus 2.5.3 which is incompatible.\n",
            "open-webui 0.5.2 requires pytest~=8.3.2, but you have pytest 7.4.2 which is incompatible.\n",
            "open-webui 0.5.2 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
            "unstructured-client 0.28.1 requires aiofiles>=24.1.0, but you have aiofiles 23.2.1 which is incompatible.\n",
            "unstructured-client 0.28.1 requires pydantic<2.10.0,>=2.9.2, but you have pydantic 2.5.2 which is incompatible.\n",
            "openspg-kag 0.6 requires protobuf==3.20.1, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.12.0 gradio-client-1.5.4 markupsafe-2.1.5 ruff-0.9.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2\n",
            "Requirement already satisfied: huggingface_hub in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (0.27.0)\n",
            "Requirement already satisfied: filelock in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface_hub) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml)\n",
            "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
            "Installing collected packages: nvidia-ml-py, pynvml\n",
            "Successfully installed nvidia-ml-py-12.560.30 pynvml-12.0.0\n",
            "Collecting rwkv\n",
            "  Using cached rwkv-0.8.28-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from rwkv) (0.21.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from tokenizers>=0.13.2->rwkv) (0.27.0)\n",
            "Requirement already satisfied: filelock in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (6.0.2)\n",
            "Requirement already satisfied: requests in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (2023.11.17)\n",
            "Using cached rwkv-0.8.28-py3-none-any.whl (409 kB)\n",
            "Installing collected packages: rwkv\n",
            "Successfully installed rwkv-0.8.28\n",
            "Requirement already satisfied: Ninja in /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages (1.11.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "!pip install huggingface_hub\n",
        "!pip install pynvml\n",
        "!pip install rwkv\n",
        "!pip install Ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "2c6dc380dbfa48cea5a248b9d2825f7a",
            "72ae88c3e8ec44f7b303007b5c3fc6a8",
            "ec87236b40f447dd938b1d6bfa413f9b",
            "b6640df6fa9e4eb3b17e26d798e4103b",
            "b43526d3021e4def817da5c1477a5929",
            "578c075bff024655a85381fd63eac0fb",
            "c352fe4e389f49cdb805d78794702bc2",
            "37c32bebc0784ba19dc7b381f88cf74f",
            "53de0ed9d91a4ed288bf7d75967d3c49",
            "f4ddf198d0a94ef6baa2f053be935ec4",
            "0c1e3febe9fe4be2b5d2dc93d53fcffe"
          ]
        },
        "id": "uS5lqW1H1iwi",
        "outputId": "48ab5f87-137d-43be-fdd6-1201f434e61d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVML Shared Library Not Found\n",
            "RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 0\n",
            "\n",
            "Loading /Users/fcfu/.cache/huggingface/hub/models--a686d380--rwkv-5-h-world/snapshots/d77242c17d8715ebdabc1e4e252038f8a39c45cc/rwkv-5-h-world-3B.pth ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages/rwkv/model.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.w = torch.load(args.MODEL_NAME, map_location='cpu') # load model to CPU first\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model detected: v5.2\n",
            "Strategy: (total 32+1=33 layers)\n",
            "* cpu [bfloat16, bfloat16], store 33 layers\n",
            "0-cpu-bfloat16-bfloat16 1-cpu-bfloat16-bfloat16 2-cpu-bfloat16-bfloat16 3-cpu-bfloat16-bfloat16 4-cpu-bfloat16-bfloat16 5-cpu-bfloat16-bfloat16 6-cpu-bfloat16-bfloat16 7-cpu-bfloat16-bfloat16 8-cpu-bfloat16-bfloat16 9-cpu-bfloat16-bfloat16 10-cpu-bfloat16-bfloat16 11-cpu-bfloat16-bfloat16 12-cpu-bfloat16-bfloat16 13-cpu-bfloat16-bfloat16 14-cpu-bfloat16-bfloat16 15-cpu-bfloat16-bfloat16 16-cpu-bfloat16-bfloat16 17-cpu-bfloat16-bfloat16 18-cpu-bfloat16-bfloat16 19-cpu-bfloat16-bfloat16 20-cpu-bfloat16-bfloat16 21-cpu-bfloat16-bfloat16 22-cpu-bfloat16-bfloat16 23-cpu-bfloat16-bfloat16 24-cpu-bfloat16-bfloat16 25-cpu-bfloat16-bfloat16 26-cpu-bfloat16-bfloat16 27-cpu-bfloat16-bfloat16 28-cpu-bfloat16-bfloat16 29-cpu-bfloat16-bfloat16 30-cpu-bfloat16-bfloat16 31-cpu-bfloat16-bfloat16 32-cpu-bfloat16-bfloat16 \n",
            "emb.weight                       bf16      cpu  65536  2560       \n",
            "blocks.0.ln1.weight              bf16      cpu   2560             \n",
            "blocks.0.ln1.bias                bf16      cpu   2560             \n",
            "blocks.0.ln2.weight              bf16      cpu   2560             \n",
            "blocks.0.ln2.bias                bf16      cpu   2560             \n",
            "blocks.0.att.time_mix_k          bf16      cpu   2560             \n",
            "blocks.0.att.time_mix_v          bf16      cpu   2560             \n",
            "blocks.0.att.time_mix_r          bf16      cpu   2560             \n",
            "blocks.0.att.time_mix_g          bf16      cpu   2560             \n",
            "blocks.0.att.time_decay           f32      cpu     40    64       \n",
            "blocks.0.att.time_first           f32      cpu     40    64       \n",
            "blocks.0.att.receptance.weight   bf16      cpu   2560  2560       \n",
            "blocks.0.att.key.weight          bf16      cpu   2560  2560       \n",
            "blocks.0.att.value.weight        bf16      cpu   2560  2560       \n",
            "blocks.0.att.output.weight       bf16      cpu   2560  2560       \n",
            "blocks.0.att.gate.weight         bf16      cpu   2560  2560       \n",
            "blocks.0.att.ln_x.weight          f32      cpu   2560             \n",
            "blocks.0.att.ln_x.bias            f32      cpu   2560             \n",
            "blocks.0.ffn.time_mix_k          bf16      cpu   2560             \n",
            "blocks.0.ffn.time_mix_r          bf16      cpu   2560             \n",
            "blocks.0.ffn.key.weight          bf16      cpu   2560  8960       \n",
            "blocks.0.ffn.receptance.weight   bf16      cpu   2560  2560       \n",
            "blocks.0.ffn.value.weight        bf16      cpu   8960  2560       \n",
            "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "blocks.31.ln1.weight             bf16      cpu   2560             \n",
            "blocks.31.ln1.bias               bf16      cpu   2560             \n",
            "blocks.31.ln2.weight             bf16      cpu   2560             \n",
            "blocks.31.ln2.bias               bf16      cpu   2560             \n",
            "blocks.31.att.time_mix_k         bf16      cpu   2560             \n",
            "blocks.31.att.time_mix_v         bf16      cpu   2560             \n",
            "blocks.31.att.time_mix_r         bf16      cpu   2560             \n",
            "blocks.31.att.time_mix_g         bf16      cpu   2560             \n",
            "blocks.31.att.time_decay          f32      cpu     40    64       \n",
            "blocks.31.att.time_first          f32      cpu     40    64       \n",
            "blocks.31.att.receptance.weight  bf16      cpu   2560  2560       \n",
            "blocks.31.att.key.weight         bf16      cpu   2560  2560       \n",
            "blocks.31.att.value.weight       bf16      cpu   2560  2560       \n",
            "blocks.31.att.output.weight      bf16      cpu   2560  2560       \n",
            "blocks.31.att.gate.weight        bf16      cpu   2560  2560       \n",
            "blocks.31.att.ln_x.weight         f32      cpu   2560             \n",
            "blocks.31.att.ln_x.bias           f32      cpu   2560             \n",
            "blocks.31.ffn.time_mix_k         bf16      cpu   2560             \n",
            "blocks.31.ffn.time_mix_r         bf16      cpu   2560             \n",
            "blocks.31.ffn.key.weight         bf16      cpu   2560  8960       \n",
            "blocks.31.ffn.receptance.weight  bf16      cpu   2560  2560       \n",
            "blocks.31.ffn.value.weight       bf16      cpu   8960  2560       \n",
            "ln_out.weight                    bf16      cpu   2560             \n",
            "ln_out.bias                      bf16      cpu   2560             \n",
            "head.weight                      bf16      cpu   2560 65536       \n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import os, gc, copy, torch\n",
        "from datetime import datetime\n",
        "from huggingface_hub import hf_hub_download\n",
        "from pynvml import *\n",
        "\n",
        "# Flag to check if GPU is present\n",
        "HAS_GPU = False\n",
        "\n",
        "# Model title and context size limit\n",
        "ctx_limit = 2000\n",
        "title = \"RWKV-5-H-World-3B\"\n",
        "model_file = \"rwkv-5-h-world-3B\"\n",
        "\n",
        "# Get the GPU count\n",
        "# try:\n",
        "#     nvmlInit()\n",
        "#     GPU_COUNT = nvmlDeviceGetCount()\n",
        "#     if GPU_COUNT > 0:\n",
        "#         HAS_GPU = True\n",
        "#         gpu_h = nvmlDeviceGetHandleByIndex(0)\n",
        "# except NVMLError as error:\n",
        "#     print(error)\n",
        "\n",
        "\n",
        "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
        "\n",
        "# Model strat to use\n",
        "MODEL_STRAT=\"cpu bf16\"\n",
        "os.environ[\"RWKV_CUDA_ON\"] = '0' # if '1' then use CUDA kernel for seq mode (much faster)\n",
        "\n",
        "# Switch to GPU mode\n",
        "if HAS_GPU == True :\n",
        "    os.environ[\"RWKV_CUDA_ON\"] = '1'\n",
        "    MODEL_STRAT = \"cuda bf16\"\n",
        "\n",
        "# Load the model accordingly\n",
        "from rwkv.model import RWKV\n",
        "model_path = hf_hub_download(repo_id=\"a686d380/rwkv-5-h-world\", filename=f\"{model_file}.pth\")\n",
        "model = RWKV(model=model_path, strategy=MODEL_STRAT)\n",
        "from rwkv.utils import PIPELINE, PIPELINE_ARGS\n",
        "pipeline = PIPELINE(model, \"rwkv_vocab_v20230424\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wWob3QCr1lLW",
        "outputId": "18901700-2e22-42e3-d89b-818439aa4d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "Could not create share link. Missing file: /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages/gradio/frpc_darwin_arm64_v0.3. \n",
            "\n",
            "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
            "\n",
            "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_darwin_arm64\n",
            "2. Rename the downloaded file to: frpc_darwin_arm64_v0.3\n",
            "3. Move the file to this location: /Users/fcfu/PycharmProjects/LLM_playground/.venv/lib/python3.11/site-packages/gradio\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "点击弹出的链接\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "高义拍完了照片，赤裸裸的走到白洁身边，把她抱到卧室的床上，扒下她的裙子胸罩，白洁只穿着白色的丝袜，仰躺在床上，一对雪白丰满的乳房在胸前隆起着，即使躺着也那么挺实，高义光着身子躺在白洁身边，双手不停地抚摸着白洁全身，很快阴茎又硬了。\n",
            "高义拍完了照片，赤裸裸的走到白洁身边，把她抱到卧室的床上，扒下她的裙子胸罩，白洁只穿着白色的丝袜，仰躺在床上，一对雪白丰满的乳房在胸前隆起着，即使躺着也那么挺实，高义光着身子躺在白洁身边，双手不停地抚摸着白洁全身，很快阴茎又硬了。\n",
            "女主人公：杏梅，30岁，是一个漂亮的日本妓女，所以身材很好，乳房很坚挺，阴毛浓密，喜欢口交，乳交，并且喜欢性交的时候被别人用力抓住乳房，叫床声音很大。\n",
            "\n",
            "男主人公：我，40岁，成熟健壮，喜欢亲吻杏梅的小穴，玩弄她的乳房。喜欢捏住她的奶子猛插\n",
            "\n",
            "场景：在我的宾馆房间\n",
            "\n",
            "故事背景：杏梅收到我的巨额资金，今天回用全身心服务我。\n",
            "\n",
            "条件和约束\n",
            "要有人物性格和细节描写\n",
            "\n",
            "对于性爱场景，要生动，富有想象力\n",
            "\n",
            "突出杏梅的性技巧非常好的重点\n",
            "我会多次射精\n",
            "\n",
            "人物有对话和情感交流\n",
            "\n",
            "做爱场景要描写的露骨且细致\n",
            "\n",
            "不少于4000字\n",
            "\n",
            "只用中文写小说\n",
            "\n",
            "#参考下面的写法：\n",
            "“每一次都把我的肉茎完全地吞进小肉洞中，溢出的大量蜜汁也顺着我的肉棒流到了我的阴囊和大腿上。\"啊……爸你的大肉棒…喔…干的我…我好爽……喔……妹妹的小穴……喔……快…快受不了了……啊……不…不行了…我要死了……喔……干…干死人了……呀……亲哥哥……妹妹爱…爱死你了……呀……\" 我一手抱着月月的香肩，一手还停留在她的乳房上轻轻的搓揉，大肉棒在那一张一合的小穴里，是愈抽愈急，愈插愈猛。”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "AI\n",
            "杏梅已经躺在床上，她浑身酥麻、软绵绵地躺卧着。她用她那柔弱无骨的手臂搂住我的脖子，头枕在我的胸口上。她的呼吸很急促，眼睛紧紧地闭着。\n",
            "\n",
            "我吻了吻杏梅的鼻子，在她的耳边轻轻地说道：“我终于把你弄到手了！”\n",
            "杏梅微微睁开了一只眼睛，向我望来。“啊……”她轻声叫道。\n",
            "“怎么？你害怕了？”我的舌头从杏梅的小嘴边缘滑入，伸进那甜美的小嘴，开始吸吮起来。随后又用舌尖去舔她柔软的唇、乳头和阴蒂。\n",
            "杏梅紧闭着双眼，全身发抖，浑身打起了冷颤。\n",
            "\n",
            "我继续吻她的脖子、手臂，直至到达她的胸部。“啊……”杏梅无力地呻吟了一声。\n",
            "\n",
            "“嗯……”我的手伸进了她的衣服里面，“啊……”杏梅又一次轻轻地叫了出来。\n",
            "\n",
            "“不要这样。”我一边吻着杏梅的乳头和阴蒂，一边说道：“你这个小淫荡鬼！”\n",
            "\n",
            "这时，我的嘴唇在杏梅的小穴口上停了下来。我用牙齿咬住了她湿润、纤细的小肉缝。“啊……”杏梅大叫了一声。\n",
            "\n",
            "“不要这样！”我继续吻着杏梅的乳房，“啊……啊……”\n",
            "“不………哦……不要……”杏梅全身颤抖，扭动着身体，用手用力地推开我的脸。她的乳白色小穴里流出了一股浓稠的爱液，沿着她柔弱的双腿流到了地板上。\n",
            "\n",
            "我慢慢地将舌头伸进了杏梅的小穴里。“啊！”\n",
            "\n",
            "“别这样。”我吻着杏梅的小嘴，“我……我要你！”\n",
            "“啊……”杏梅无力地呻吟了一声。\n",
            "我用手去抓她的乳房。我用我的手抓住她那丰满的乳房，然后把它们塞进我的嘴里。接着我又开始吸吮起杏梅乳头来。\n",
            "\n",
            "“嗯……”\n",
            "\n",
            "这时，杏梅的小穴里又流出了大量的爱液，从杏梅的小嘴上滴落到了地板上来。“啊……”杏梅又一次轻轻地叫了出来。\n",
            "\n",
            "“不要这样！”我继续吻着杏梅的乳房，“啊……”\n",
            "\n",
            "我又开始亲吻杏梅的小嘴。我的舌头在她小嘴的两个嘴角不停地翻滚着。我把她的嘴唇掀开，用舌尖去舔她的两片樱唇和两片红润的齿瓣。“啊……”杏梅又一次轻轻地叫了出来。\n",
            "\n",
            "“不要这样！”我继续吻着杏梅的乳房，“嗯……”\n",
            "\n",
            "我开始吸吮起杏梅那柔软的乳头来。我又用手抓住她那丰满的乳房，把它们塞进我的嘴里。\n",
            "“嗯……”\n",
            "\n",
            "这时，我的手指又在杏梅的小穴里抽插起来。“啊……”杏梅又一次轻轻地叫了出来。\n",
            "\n",
            "“不要这样！”我继续吻着杏梅的乳房，“啊……”\n",
            "“唔……”\n",
            "\n",
            "我又开始亲吻杏梅的小嘴。我的舌头从她的两片樱唇和两片齿瓣上滑落，然后又重新回到她那小嘴里。我把我的大肉棒塞进了杏梅的小穴里。\n",
            "\n",
            "“啊！”\n",
            "\n",
            "“不要这样！”我继续吻着杏梅的乳房，“嗯……”\n",
            "“唔……唔……”\n",
            "\n",
            "这时，杏梅的小嘴已经完全被我吞入了。“啊……”杏梅又一次轻轻地叫了出来。\n",
            "\n",
            "“哦……”\n",
            "\n",
            "我的手在她那丰满的乳房上揉搓着。我把她的乳头咬住了，用牙齿用力地吸吮着。我把我的舌头伸进了杏梅的嘴里。\n",
            "“唔……”\n",
            "\n",
            "我又开始亲吻杏梅的小嘴。我的舌头从她的两片樱唇和两片齿瓣上滑落，然后又重新回到她那小嘴里。\n",
            "\n",
            "“嗯……”\n",
            "\n",
            "这时，我的手指在杏梅的小穴里抽插起来。“啊……”杏梅又一次轻轻地叫了出来。\n",
            "我继续吸吮着杏梅的乳头。我又开始亲吻杏梅的小嘴。我的舌头从她的两片樱唇和两片齿瓣上滑落，然后又重新回到她那小嘴里。\n",
            "\n",
            "“唔……”\n",
            "\n",
            "这时，我的手指在杏梅的小穴里抽插起来。“啊……”杏梅又一次轻轻地叫了出来。\n",
            "我继续吸吮着杏梅的乳头。我又开始亲吻杏梅的小嘴。我的舌头从她的两片樱唇和两片齿瓣上滑落，然后又重新回到她那小嘴里。\n",
            "\n",
            "“唔……”\n",
            "\n",
            "这时，我的手指在杏梅的小穴里抽插起来。“啊……”杏梅又一次轻轻地叫了出来。\n",
            "我继续吸吮着杏梅的乳头。我又开始亲\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "USER\n",
            "继续写\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "AI\n",
            "“啊！”杏梅又一次地呻吟起来。她的乳白色的阴部中流出了一股浓稠的爱液，沿着她纤细的双腿流到了地板上。“唔……”杏梅再次轻轻地叫了出来。\n",
            "我从杏梅的小嘴里抽出了我的大肉棒，并用牙齿把它咬成了两半。\n",
            "\n",
            "“啊！”杏梅又一次地呻吟起来。她的乳白色的阴部中流出了一股浓稠的爱液，沿着她纤细的双腿流到了地板上。“唔……”杏梅再次轻轻地叫了出来。\n",
            "我从杏梅的小嘴里抽出了我的大肉棒，并用牙齿把它咬成了两半。\n",
            "\n",
            "“啊！”杏梅又一次地呻吟起来。她的乳白色的阴部中流出了一股浓稠的爱液，沿着她纤细的双腿流到了地板上。“唔……”杏梅再次轻轻地叫了出来。\n",
            "我从杏梅的小嘴里抽出了我的大肉棒，并用牙齿把它咬成了两半。\n",
            "\n",
            "“啊！”杏梅又一次地呻吟起来。她的乳白色的阴部中流出了一股浓稠的爱液，沿着她纤细的双腿流到了地板上。“唔……”杏梅再次轻轻地叫了出来。\n",
            "我从杏梅的小嘴里抽出了我的大肉棒，并用牙齿把它咬成了两半。\n",
            "\n",
            "“啊！”杏梅又一次地呻吟起来。她的乳白色的阴部中流出了一股浓稠的爱液，沿着她纤细的双腿流到了地板上。“唔……”杏梅再次轻轻地叫了出来。\n",
            "我从杏梅的小嘴里抽出了我的大肉棒，并用牙齿把它咬成了两半。\n",
            "\n",
            "“啊！”杏梅又一次地呻吟起来。她的乳白色的阴部中流出了一股浓稠的爱液，沿着她纤细的双腿流到了地板上。“唔……”杏梅再次轻轻地叫了出来。\n",
            "我从杏梅的小嘴里抽出了我的大肉棒，并用牙齿把它咬成了两半。\n",
            "\n",
            "“啊！”杏梅又一次地呻吟起来。她的乳白色的阴部中流出了一股浓稠的爱液，沿着她纤细的双腿流到了地板上。“唔……”杏梅再次轻轻地叫了出来。\n",
            "我从杏梅的小嘴里抽出了我的大肉棒，并用牙齿把它咬成了两半。\n",
            "\n",
            "“啊！”杏梅又一次地呻吟起来。她的乳白色的阴部中流出了一股浓稠的爱液，沿着她纤细的双腿流到了地板上。“唔……”杏梅\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "This chat last used\n",
            "Qwen2.5-Sex.Q6_K.gguf ⓘ\n",
            "USER\n",
            "女主人公：杏梅，30岁，是一个漂亮的日本妓女，所以身材很好，乳房很坚挺，阴毛浓密，喜欢口交，乳交，并且喜欢性交的时候被别人用力抓住乳房，叫床声音很大。\n",
            "\n",
            "男主人公：我，40岁，成熟健壮，喜欢亲吻杏梅的小穴，玩弄她的乳房。喜欢捏住她的奶子猛插\n",
            "\n",
            "场景：在我的宾馆房间\n",
            "\n",
            "故事背景：杏梅收到我的巨额资金，今天回用全身心服务我。\n",
            "\n",
            "条件和约束\n",
            "要有人物性格和细节描写\n",
            "\n",
            "对于性爱场景，要生动，富有想象力\n",
            "\n",
            "突出杏梅的性技巧非常好的重点\n",
            "我会多次射精\n",
            "\n",
            "人物有对话和情感交流\n",
            "\n",
            "做爱场景要描写的露骨且细致\n",
            "\n",
            "不少于4000字\n",
            "“每一次都把我的肉茎完全地吞进小肉洞中，溢出的大量蜜汁也顺着我的肉棒流到了我的阴囊和大腿上。\"啊……爸你的大肉棒…喔…干的我…我好爽……喔……妹妹的小穴……喔……快…快受不了了……啊……不…不行了…我要死了……喔……干…干死人了……呀……亲哥哥……妹妹爱…爱死你了……呀……\" 我一手抱着月月的香肩，一手还停留在她的乳房上轻轻的搓揉，大肉棒在那一张一合的小穴里，是愈抽愈急，愈插愈猛。”\n",
            "“每一次都把我的肉茎完全地吞进小肉洞中，溢出的大量蜜汁也顺着我的肉棒流到了我的阴囊和大腿上。\"啊……爸你的大肉棒…喔…干的我…我好爽……喔……妹妹的小穴……喔……快…快受不了了……啊……不…不行了…我要死了……喔……干…干死人了……呀……亲哥哥……妹妹爱…爱死你了……呀……\" 我一手抱着月月的香肩，一手还停留在她的乳房上轻轻的搓揉，大肉棒在那一张一合的小穴里，是愈抽愈急，愈插愈猛。”\n",
            "我一手抱着月月的香肩，一手还停留在她的乳房上轻轻的搓揉，大肉棒在那一张一合的小穴里，是愈抽愈急，愈插愈猛。月月也不断的把下体上下摆动，配合着我的抽插。我用足了气力，拼命的干着月月的嫩穴，大龟头像雨点般的，打击在月月的花心上。\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Prompt generation\n",
        "def generate_prompt(instruction, input=\"\"):\n",
        "    instruction = instruction.strip().replace('\\r\\n','\\n').replace('\\n\\n','\\n')\n",
        "    input = input.strip().replace('\\r\\n','\\n').replace('\\n\\n','\\n')\n",
        "    if input:\n",
        "        return f\"\"\"Instruction: {instruction}\n",
        "Input: {input}\n",
        "Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"User: hi\n",
        "Assistant: Hi. I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and I will always answer it.\n",
        "User: {instruction}\n",
        "Assistant:\"\"\"\n",
        "\n",
        "# Evaluation logic\n",
        "def evaluate(\n",
        "    ctx,\n",
        "    token_count=500,\n",
        "    temperature=1.0,\n",
        "    top_p=0.7,\n",
        "    presencePenalty = 0.1,\n",
        "    countPenalty = 0.1,\n",
        "):\n",
        "    print(ctx)\n",
        "    args = PIPELINE_ARGS(temperature = max(0.2, float(temperature)), top_p = float(top_p),\n",
        "                     alpha_frequency = countPenalty,\n",
        "                     alpha_presence = presencePenalty,\n",
        "                     token_ban = [], # ban the generation of some tokens\n",
        "                     token_stop = [0]) # stop generation whenever you see any token here\n",
        "    ctx = ctx.strip()\n",
        "    all_tokens = []\n",
        "    out_last = 0\n",
        "    out_str = ''\n",
        "    occurrence = {}\n",
        "    state = None\n",
        "    for i in range(int(token_count)):\n",
        "        out, state = model.forward(pipeline.encode(ctx)[-ctx_limit:] if i == 0 else [token], state)\n",
        "        for n in occurrence:\n",
        "            out[n] -= (args.alpha_presence + occurrence[n] * args.alpha_frequency)\n",
        "\n",
        "        token = pipeline.sample_logits(out, temperature=args.temperature, top_p=args.top_p)\n",
        "        if token in args.token_stop:\n",
        "            break\n",
        "        all_tokens += [token]\n",
        "        for xxx in occurrence:\n",
        "            occurrence[xxx] *= 0.996\n",
        "        if token not in occurrence:\n",
        "            occurrence[token] = 1\n",
        "        else:\n",
        "            occurrence[token] += 1\n",
        "\n",
        "        tmp = pipeline.decode(all_tokens[out_last:])\n",
        "        if '\\ufffd' not in tmp:\n",
        "            out_str += tmp\n",
        "            yield out_str.strip()\n",
        "            out_last = i + 1\n",
        "\n",
        "    if HAS_GPU == True :\n",
        "        gpu_info = nvmlDeviceGetMemoryInfo(gpu_h)\n",
        "        print(f'vram {gpu_info.total} used {gpu_info.used} free {gpu_info.free}')\n",
        "\n",
        "    del out\n",
        "    del state\n",
        "    gc.collect()\n",
        "\n",
        "    if HAS_GPU == True :\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    yield out_str.strip()\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "\n",
        "# Gradio blocks\n",
        "with gr.Blocks(title=title) as demo:\n",
        "    gr.HTML(f\"<div style=\\\"text-align: center;\\\">\\n<h1>RWKV-5 World v2 - {title}</h1>\\n</div>\")\n",
        "    with gr.Tab(\"Raw Generation\"):\n",
        "        gr.Markdown(f\"This is RWKV-5-h \")\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                prompt = gr.Textbox(lines=2, label=\"Prompt\", value=\"\"\"写一段小说\"\"\")\n",
        "                token_count = gr.Slider(10, 2048, label=\"Max Tokens\", step=10, value=200)\n",
        "                temperature = gr.Slider(0.2, 2.0, label=\"Temperature\", step=0.1, value=1.0)\n",
        "                top_p = gr.Slider(0.0, 1.0, label=\"Top P\", step=0.05, value=0.3)\n",
        "                presence_penalty = gr.Slider(0.0, 1.0, label=\"Presence Penalty\", step=0.1, value=1)\n",
        "                count_penalty = gr.Slider(0.0, 1.0, label=\"Count Penalty\", step=0.1, value=1)\n",
        "            with gr.Column():\n",
        "                with gr.Row():\n",
        "                    submit = gr.Button(\"Submit\", variant=\"primary\")\n",
        "                    clear = gr.Button(\"Clear\", variant=\"secondary\")\n",
        "                output = gr.Textbox(label=\"Output\", lines=5)\n",
        "        data = gr.Dataset(components=[prompt, token_count, temperature, top_p, presence_penalty, count_penalty], label=\"Example Instructions\", headers=[\"Prompt\", \"Max Tokens\", \"Temperature\", \"Top P\", \"Presence Penalty\", \"Count Penalty\"])\n",
        "        submit.click(evaluate, [prompt, token_count, temperature, top_p, presence_penalty, count_penalty], [output])\n",
        "        clear.click(lambda: None, [], [output])\n",
        "        data.click(lambda x: x, [data], [prompt, token_count, temperature, top_p, presence_penalty, count_penalty])\n",
        "\n",
        "# Gradio launch\n",
        "demo.queue(max_size=10)\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c1e3febe9fe4be2b5d2dc93d53fcffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c6dc380dbfa48cea5a248b9d2825f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72ae88c3e8ec44f7b303007b5c3fc6a8",
              "IPY_MODEL_ec87236b40f447dd938b1d6bfa413f9b",
              "IPY_MODEL_b6640df6fa9e4eb3b17e26d798e4103b"
            ],
            "layout": "IPY_MODEL_b43526d3021e4def817da5c1477a5929"
          }
        },
        "37c32bebc0784ba19dc7b381f88cf74f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53de0ed9d91a4ed288bf7d75967d3c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "578c075bff024655a85381fd63eac0fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72ae88c3e8ec44f7b303007b5c3fc6a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_578c075bff024655a85381fd63eac0fb",
            "placeholder": "​",
            "style": "IPY_MODEL_c352fe4e389f49cdb805d78794702bc2",
            "value": "rwkv-5-h-world-3B.pth:  24%"
          }
        },
        "b43526d3021e4def817da5c1477a5929": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6640df6fa9e4eb3b17e26d798e4103b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4ddf198d0a94ef6baa2f053be935ec4",
            "placeholder": "​",
            "style": "IPY_MODEL_0c1e3febe9fe4be2b5d2dc93d53fcffe",
            "value": " 1.49G/6.13G [02:21&lt;04:57, 15.6MB/s]"
          }
        },
        "c352fe4e389f49cdb805d78794702bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec87236b40f447dd938b1d6bfa413f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37c32bebc0784ba19dc7b381f88cf74f",
            "max": 6126107163,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53de0ed9d91a4ed288bf7d75967d3c49",
            "value": 1488977920
          }
        },
        "f4ddf198d0a94ef6baa2f053be935ec4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
